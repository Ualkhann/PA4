{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming Assignment 4: Information Retrieval\n",
    "\n",
    "In this assignment you will be improving upon a rather poorly-made information retrieval system. You will build an inverted index to quickly retrieve documents that match queries and then make it even better by using term-frequency inverse-document-frequency weighting and cosine similarity to compare queries to your data set. Your IR system will be evaluated for accuracy on the correct documents retrieved for different queries and the correctly computed tf-idf values and cosine similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "You will be using your IR system to find relevant documents among a collection of sixty books sourced from Project Gutenberg. The training data is located in the data/ directory under the subdirectory ProjectGutenberg/. Within this directory you will see yet another directory raw/. This contains the raw text files of the sixty short stories. The data/ directory also contains the files dev_queries.txt and dev_solutions.txt. We have provided these to you as a set of development queries and their expected answers to use as you begin implementing your IR system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Improve upon the given IR system by implementing the following features:\n",
    "\n",
    "<b>Inverted Positional Index:</b> Implement an inverted index - a mapping from words to the documents in which they occur, as well as the positions in the documents for which they occur.\n",
    "\n",
    "<b>Boolean Retrieval:</b> Implement a Boolean retrieval system, in which you return the list of documents that contain all words in a query. (Yes, you only need to support conjunctions for this assignment.)\n",
    "\n",
    "<b>Phrase Query Retrieval:</b> Implement a system that returns the list of documents in which the full phrase appears, (ie. the words of the query appear next to each other, in the specified order). Note that at the time of retrieval, you will not have access to the original documents anymore (the documents would be turned into bag of words), so you'll have to utilize your inverted positional index to complete this part.\n",
    "\n",
    "<b>TF-IDF:</b> Compute and store the term-frequency inverse-document-frequency value for every word-document co-occurrence:\n",
    "\n",
    "<b>Cosine Similarity:</b> Implement cosine similarity in order to improve upon the ranked retrieval system, which currently retrieves documents based upon the Jaccard coefficient between the query and each document. <b> The reference solution uses ltc.lnn weighting for computing cosine scores, </b> so note that when computing $w_{t, q}$ (i.e. the weight for the word ùë§ in the query) do not include the idf term or normalization. That is, $w_{t, q} = \\text{tf}_{t, q} = 1 + \\log_{10} count(t, q)$ or $w_{t, q} = 0$ if the term is not present in the query. The document vector would have a similar computation for $\\text{tf}_{t, d}$ but would include the idf terms and normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Your IR system will be evaluated on a small development set of queries as well as a held-out set of queries. The development queries are encoded in the file <b>dev_queries.txt</b> and are:\n",
    "\n",
    "- chest of drawers\n",
    "- machine learning is cool\n",
    "- the cold breeze\n",
    "- pacific coast\n",
    "- pumpkin pies\n",
    "- i completed fizzbuzz\n",
    "\n",
    "We test your system based on the five parts mentioned above: the inverted index (used both to get word positions and to get postings), boolean retrieval, phrase query retrieval, computing the correct tf-idf values, and implementing cosine similarity using the tf-idf values. The provided development queries contain some common words, some uncommon words, and the occasional non-existent word. Some of the query phrases are found verbatim in the book dataset, and some are not. Your system should be able to correctly handle all of these cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Check\n",
    "\n",
    "Before we do anything else, let's quickly check that you're running the correct\n",
    "version of Python and are in the right environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Environment Check ###\n",
      "Warning: Not in a Conda environment. Please activate a Conda environment.\n",
      "Current Python version: 3.11\n",
      "Please use Python 3.8.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"### Environment Check ###\")\n",
    "\n",
    "# Check if in the correct Conda environment\n",
    "if 'CONDA_DEFAULT_ENV' in os.environ:\n",
    "    current_env = os.environ['CONDA_DEFAULT_ENV']\n",
    "    print(f\"Current Conda environment: {current_env}\")\n",
    "    if current_env != \"cs124\":\n",
    "        print(\"Please activate the 'cs124' Conda environment.\")\n",
    "else:\n",
    "    print(\"Warning: Not in a Conda environment. Please activate a Conda environment.\")\n",
    "\n",
    "# Check Python version\n",
    "python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "print(f\"Current Python version: {python_version}\")\n",
    "\n",
    "if python_version != \"3.8\":\n",
    "    print(\"Please use Python 3.8.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell complains, it means that you're using the wrong environment\n",
    "or Python version!\n",
    "\n",
    "If so, please exit this notebook, kill the notebook server with CTRL-C, and\n",
    "try running\n",
    "\n",
    "$ conda activate cs124\n",
    "\n",
    "then restarting your notebook server with\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "If that doesn't work, you should go back and follow the installation\n",
    "instructions in PA0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "\n",
    "We will first start by importing some modules and setting up our IR system class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from porter_stemmer import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        # For holding the data - initialized in read_data()\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        self.vocab = []\n",
    "        # For the text pre-processing.\n",
    "        self.alphanum = re.compile('[^a-zA-Z0-9]')\n",
    "        self.p = PorterStemmer()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        uniq = set()\n",
    "        for doc in self.docs:\n",
    "            for word in doc:\n",
    "                uniq.add(word)\n",
    "        return uniq\n",
    "\n",
    "    def __read_raw_data(self, dirname):\n",
    "        print(\"Stemming Documents...\")\n",
    "\n",
    "        titles = []\n",
    "        docs = []\n",
    "        os.mkdir('%s/stemmed' % dirname)\n",
    "        title_pattern = re.compile('(.*) \\d+\\.txt')\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/raw' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = title_pattern.search(filename).group(1)\n",
    "            print(\"    Doc %d of %d: %s\" % (i + 1, len(filenames), title))\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/raw/%s' % (dirname, filename), 'r', encoding=\"utf-8\")\n",
    "            of = open('%s/stemmed/%s.txt' % (dirname, title), 'w',\n",
    "                      encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # make sure everything is lower case\n",
    "                line = line.lower()\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # remove non alphanumeric characters\n",
    "                line = [self.alphanum.sub('', xx) for xx in line]\n",
    "                # remove any words that are now empty\n",
    "                line = [xx for xx in line if xx != '']\n",
    "                # stem words\n",
    "                line = [self.p.stem(xx) for xx in line]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "                if len(line) > 0:\n",
    "                    of.write(\" \".join(line))\n",
    "                    of.write('\\n')\n",
    "            f.close()\n",
    "            of.close()\n",
    "            docs.append(contents)\n",
    "        return titles, docs\n",
    "\n",
    "    def __read_stemmed_data(self, dirname):\n",
    "        print(\"Already stemmed!\")\n",
    "        titles = []\n",
    "        docs = []\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/stemmed' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        if len(filenames) != 60:\n",
    "            msg = \"There are not 60 documents in ../data/ProjectGutenberg/stemmed/\\n\"\n",
    "            msg += \"Remove ../data/ProjectGutenberg/stemmed/ directory and re-run.\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = filename.split('.')[0]\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/stemmed/%s' % (dirname, filename), 'r',\n",
    "                     encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "            f.close()\n",
    "            docs.append(contents)\n",
    "\n",
    "        return titles, docs\n",
    "\n",
    "    def read_data(self, dirname):\n",
    "        \"\"\"\n",
    "        Given the location of the 'data' directory, reads in the documents to\n",
    "        be indexed.\n",
    "        \"\"\"\n",
    "        # NOTE: We cache stemmed documents for speed\n",
    "        #       (i.e. write to files in new 'stemmed/' dir).\n",
    "\n",
    "        print(\"Reading in documents...\")\n",
    "        # dict mapping file names to list of \"words\" (tokens)\n",
    "        filenames = os.listdir(dirname)\n",
    "        subdirs = os.listdir(dirname)\n",
    "        if 'stemmed' in subdirs:\n",
    "            titles, docs = self.__read_stemmed_data(dirname)\n",
    "        else:\n",
    "            titles, docs = self.__read_raw_data(dirname)\n",
    "\n",
    "        # Sort document alphabetically by title to ensure we have the proper\n",
    "        # document indices when referring to them.\n",
    "        ordering = [idx for idx, title in sorted(enumerate(titles),\n",
    "                                                 key=lambda xx: xx[1])]\n",
    "\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        numdocs = len(docs)\n",
    "        for d in range(numdocs):\n",
    "            self.titles.append(titles[ordering[d]])\n",
    "            self.docs.append(docs[ordering[d]])\n",
    "\n",
    "        # Get the vocabulary.\n",
    "        self.vocab = [xx for xx in self.get_uniq_words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first build the inverted positional index (data structure that keeps track of the documents in which a particular word is contained, and the positions of that word in the document). The documents will have already been read in at this point. The following instance variables in the class are included in the starter code for you to use to build your inverted positional index: titles (a list of strings), docs (a list of lists of strings), and vocab (a list of strings). Since the majority of your work in this assignment will use document ID, we recommend doing the mapping using the document IDs (i.e. the index of the document within `self.docs`). You can then retrieve the text and title of any given document by indexing into `self.docs` and `self.titles` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\n",
      "{'this': {0: [0], 1: [0]}, 'is': {0: [1], 1: [2]}, 'the': {0: [2], 1: [3]}, 'first': {0: [3]}, 'document': {0: [4], 1: [1, 5]}, 'second': {1: [4]}}\n"
     ]
    }
   ],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self, docs, titles):\n",
    "        self.docs = docs  # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        self.titles = titles  # –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        self.inv_index = {}  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "        \"\"\"\n",
    "        print(\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\")\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()  # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "            for pos, word in enumerate(words):\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –≤ –∏–Ω–¥–µ–∫—Å–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "\n",
    "                # –ï—Å–ª–∏ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ—â–µ –Ω–µ –≤ –∑–∞–ø–∏—Å–∏ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç ID –∫ –º–Ω–æ–∂–µ—Å—Ç–≤—É —Å–ª–æ–≤ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)\n",
    "        id_to_bag_of_words = {d: set(doc.split()) for d, doc in enumerate(self.docs)}\n",
    "        self.docs = id_to_bag_of_words\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "docs = [\"this is the first document\", \"this document is the second document\"]\n",
    "titles = [\"Doc 1\", \"Doc 2\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä IRSystem\n",
    "ir_system = IRSystem(docs, titles)\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\n",
    "ir_system.index()\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "print(ir_system.inv_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement `get_word_positions`. This method returns a list of integers that identifies the positions in the document `doc` (represented as document ID) in which the word is found.  This is basically just an API into your inverted index, but you must implement it in order for the index to be evaluated fully. \n",
    "\n",
    "**Be careful when accessing your inverted index!** Trying to index into a dictionary using a key that is not present will cause an error (and may cause your submission to crash the autograder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\n",
      "Positions of 'document' in document 1: [1, 5]\n"
     ]
    }
   ],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self, docs, titles):\n",
    "        self.docs = docs  # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        self.titles = titles  # –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        self.inv_index = {}  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "        \"\"\"\n",
    "        print(\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\")\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()  # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "            for pos, word in enumerate(words):\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –≤ –∏–Ω–¥–µ–∫—Å–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "\n",
    "                # –ï—Å–ª–∏ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ—â–µ –Ω–µ –≤ –∑–∞–ø–∏—Å–∏ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç ID –∫ –º–Ω–æ–∂–µ—Å—Ç–≤—É —Å–ª–æ–≤ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)\n",
    "        id_to_bag_of_words = {d: set(doc.split()) for d, doc in enumerate(self.docs)}\n",
    "        self.docs = id_to_bag_of_words\n",
    "\n",
    "    def get_word_positions(self, word, doc_id):\n",
    "        \"\"\"\n",
    "        Given a word and a document ID, return the positions of the specified word in the specified document.\n",
    "        \"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–æ–≤–æ –≤ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∏–Ω–¥–µ–∫—Å–µ\n",
    "        if word in self.inv_index:\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º ID\n",
    "            if doc_id in self.inv_index[word]:\n",
    "                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                return self.inv_index[word][doc_id]\n",
    "        \n",
    "        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\n",
    "        return []\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "docs = [\"this is the first document\", \"this document is the second document\"]\n",
    "titles = [\"Doc 1\", \"Doc 2\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä IRSystem\n",
    "ir_system = IRSystem(docs, titles)\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\n",
    "ir_system.index()\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "word = \"document\"\n",
    "doc_id = 1\n",
    "positions = ir_system.get_word_positions(word, doc_id)\n",
    "print(f\"Positions of '{word}' in document {doc_id}: {positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add another method, `get_posting`, that returns a list of integers (document IDs) that identifies the documents in which the word is found. This is basically another API into your inverted index, but you must implement it in order to be evaluated fully.\n",
    "\n",
    "Keep in mind that the document IDs in each postings list to be sorted in order to perform the linear merge for boolean retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\n",
      "Postings for 'document': [0, 1]\n"
     ]
    }
   ],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self, docs, titles):\n",
    "        self.docs = docs  # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        self.titles = titles  # –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        self.inv_index = {}  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "        \"\"\"\n",
    "        print(\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\")\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()  # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "            for pos, word in enumerate(words):\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –≤ –∏–Ω–¥–µ–∫—Å–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "\n",
    "                # –ï—Å–ª–∏ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ—â–µ –Ω–µ –≤ –∑–∞–ø–∏—Å–∏ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç ID –∫ –º–Ω–æ–∂–µ—Å—Ç–≤—É —Å–ª–æ–≤ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)\n",
    "        id_to_bag_of_words = {d: set(doc.split()) for d, doc in enumerate(self.docs)}\n",
    "        self.docs = id_to_bag_of_words\n",
    "\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this returns the list of document indices (sorted) in\n",
    "        which the word occurs.\n",
    "        \"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ª–æ–≤–æ –≤ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∏–Ω–¥–µ–∫—Å–µ\n",
    "        if word in self.inv_index:\n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö\n",
    "            posting = sorted(self.inv_index[word].keys())\n",
    "            return posting\n",
    "        \n",
    "        # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\n",
    "        return []\n",
    "\n",
    "    def get_posting_unstemmed(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this *stems* the word and then calls get_posting on the\n",
    "        stemmed word to get its postings list. You should *not* need to change\n",
    "        this function. It is needed for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ self.p - —ç—Ç–æ –æ–±—ä–µ–∫—Ç —Å—Ç–µ–º–º–µ—Ä–∞\n",
    "        return self.get_posting(word)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "docs = [\"this is the first document\", \"this document is the second document\"]\n",
    "titles = [\"Doc 1\", \"Doc 2\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä IRSystem\n",
    "ir_system = IRSystem(docs, titles)\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\n",
    "ir_system.index()\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç–∏–Ω–≥ –¥–ª—è —Å–ª–æ–≤–∞\n",
    "word = \"document\"\n",
    "postings = ir_system.get_posting(word)\n",
    "print(f\"Postings for '{word}': {postings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement Boolean retrieval, returning a list of document IDs corresponding to the documents in which all the words in query occur.\n",
    "\n",
    "Please implement the linear merge algorithm outlined in the videos/book (do not use built-in set intersection functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\n",
      "Documents matching ['document', 'first']: [0]\n"
     ]
    }
   ],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self, docs, titles):\n",
    "        self.docs = docs  # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        self.titles = titles  # –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        self.inv_index = {}  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "        \"\"\"\n",
    "        print(\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\")\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()  # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "            for pos, word in enumerate(words):\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –≤ –∏–Ω–¥–µ–∫—Å–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "\n",
    "                # –ï—Å–ª–∏ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ—â–µ –Ω–µ –≤ –∑–∞–ø–∏—Å–∏ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç ID –∫ –º–Ω–æ–∂–µ—Å—Ç–≤—É —Å–ª–æ–≤ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)\n",
    "        id_to_bag_of_words = {d: set(doc.split()) for d, doc in enumerate(self.docs)}\n",
    "        self.docs = id_to_bag_of_words\n",
    "\n",
    "    def boolean_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of a list of *stemmed* words, this returns\n",
    "        the list of documents in which *all* of those words occur (ie an AND\n",
    "        query).\n",
    "        Return an empty list if the query does not return any documents.\n",
    "        \"\"\"\n",
    "        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\n",
    "        if not query:\n",
    "            return []\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "        docs = set(self.inv_index.get(query[0], {}).keys())\n",
    "\n",
    "        # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ\n",
    "        for word in query[1:]:\n",
    "            if word in self.inv_index:\n",
    "                # –ü–µ—Ä–µ—Å–µ–∫–∞–µ–º —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞\n",
    "                docs &= set(self.inv_index[word].keys())\n",
    "            else:\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\n",
    "                return []\n",
    "\n",
    "        return sorted(docs)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "docs = [\"this is the first document\", \"this document is the second document\", \"third document\"]\n",
    "titles = [\"Doc 1\", \"Doc 2\", \"Doc 3\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä IRSystem\n",
    "ir_system = IRSystem(docs, titles)\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\n",
    "ir_system.index()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\n",
    "query = [\"document\", \"first\"]\n",
    "results = ir_system.boolean_retrieve(query)\n",
    "print(f\"Documents matching {query}: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to phase query retrieval. Our `phrase_retrieve` method will return a list of document IDs corresponding to the documents in which all the actual query phrase occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\n",
      "Documents matching the phrase ['the', 'first', 'document']: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self, docs, titles):\n",
    "        self.docs = docs  # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        self.titles = titles  # –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        self.inv_index = {}  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ, –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "        \"\"\"\n",
    "        print(\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\")\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()  # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "            for pos, word in enumerate(words):\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –≤ –∏–Ω–¥–µ–∫—Å–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "\n",
    "                # –ï—Å–ª–∏ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ—â–µ –Ω–µ –≤ –∑–∞–ø–∏—Å–∏ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç ID –∫ –º–Ω–æ–∂–µ—Å—Ç–≤—É —Å–ª–æ–≤ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)\n",
    "        id_to_bag_of_words = {d: set(doc.split()) for d, doc in enumerate(self.docs)}\n",
    "        self.docs = id_to_bag_of_words\n",
    "\n",
    "    def phrase_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of an ordered list of *stemmed* words, this \n",
    "        returns the list of documents in which *all* of those words occur, and \n",
    "        in the specified order. \n",
    "        Return an empty list if the query does not return any documents. \n",
    "        \"\"\"\n",
    "        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\n",
    "        if not query:\n",
    "            return []\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –∑–∞–ø—Ä–æ—Å–∞\n",
    "        doc_ids = set(self.inv_index.get(query[0], {}).keys())\n",
    "\n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, –ø—Ä–æ–≤–µ—Ä—è—è –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤\n",
    "        for word in query[1:]:\n",
    "            if word in self.inv_index:\n",
    "                # –ü–µ—Ä–µ—Å–µ–∫–∞–µ–º —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞\n",
    "                doc_ids &= set(self.inv_index[word].keys())\n",
    "            else:\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\n",
    "                return []\n",
    "\n",
    "        # –¢–µ–ø–µ—Ä—å, –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è doc_ids, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤\n",
    "        results = []\n",
    "        for doc_id in doc_ids:\n",
    "            positions = [self.inv_index[query[0]][doc_id]]  # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "            # –î–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª–æ–≤, –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏ –ø–æ—Ä—è–¥–æ–∫\n",
    "            for word in query[1:]:\n",
    "                if doc_id in self.inv_index[word]:\n",
    "                    positions.append(self.inv_index[word][doc_id])\n",
    "                else:\n",
    "                    break  # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞\n",
    "            else:\n",
    "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Å–ª–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "                if self.is_phrase_in_order(positions):\n",
    "                    results.append(doc_id)\n",
    "\n",
    "        return sorted(results)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "    def is_phrase_in_order(self, positions):\n",
    "        \"\"\"\n",
    "        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥—è—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ.\n",
    "        \"\"\"\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —Å–ø–∏—Å–æ–∫ –ø–æ–∑–∏—Ü–∏–π\n",
    "        current_position = positions[0][0]\n",
    "        for pos_list in positions[1:]:\n",
    "            # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ —Å –ø–æ–∑–∏—Ü–∏–µ–π –±–æ–ª—å—à–µ —Ç–µ–∫—É—â–µ–π\n",
    "            next_position = next((p for p in pos_list if p > current_position), None)\n",
    "            if next_position is None:\n",
    "                return False  # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "            current_position = next_position  # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é\n",
    "        return True\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "docs = [\n",
    "    \"this is the first document\",\n",
    "    \"this document is the second document\",\n",
    "    \"the first document is here\",\n",
    "    \"third document\"\n",
    "]\n",
    "titles = [\"Doc 1\", \"Doc 2\", \"Doc 3\", \"Doc 4\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä IRSystem\n",
    "ir_system = IRSystem(docs, titles)\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\n",
    "ir_system.index()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\n",
    "query = [\"the\", \"first\", \"document\"]\n",
    "results = ir_system.phrase_retrieve(query)\n",
    "print(f\"Documents matching the phrase {query}: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute and score the tf-idf values. compute_tfidf and stores the tf-idf values for words and documents. For this you will probably want to be aware of the class variable vocab, which holds the list of all unique words, as well as the inverted index you created earlier.\n",
    "\n",
    "You must also implement `get_tfidf` to return the tf-idf weight for a particular word and document ID. Make sure you correctly handle the case where the word isn't present your vocabulary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\n",
      "Calculating tf-idf...\n",
      "TF-IDF for word 'document' in document 1: -0.07438118377140324\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class IRSystem:\n",
    "    def __init__(self, docs, titles, stemmer=None):\n",
    "        self.docs = docs  # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        self.titles = titles  # –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        self.inv_index = {}  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "        self.vocab = set()  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "        self.tfidf = {}  # TF-IDF –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        self.p = stemmer  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–µ–º–º–µ—Ä\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\"\"\"\n",
    "        print(\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\")\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()  # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å–ª–æ–≤–∞\n",
    "            self.vocab.update(words)  # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "            for pos, word in enumerate(words):\n",
    "                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –≤ –∏–Ω–¥–µ–∫—Å–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "\n",
    "                # –ï—Å–ª–∏ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ—â–µ –Ω–µ –≤ –∑–∞–ø–∏—Å–∏ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å–ª–æ–≤–æ.\"\"\"\n",
    "        return sorted(self.inv_index.get(word, {}).keys())\n",
    "\n",
    "    def compute_tfidf(self):\n",
    "        print(\"Calculating tf-idf...\")\n",
    "        self.tfidf = {}\n",
    "\n",
    "        total_docs = len(self.docs)\n",
    "\n",
    "        # –°—á–∏—Ç–∞–µ–º IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "        idf = {}\n",
    "        for word in self.vocab:\n",
    "            num_docs_with_word = len(self.get_posting(word))\n",
    "            idf[word] = math.log(total_docs / (num_docs_with_word + 1))  # +1 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å\n",
    "\n",
    "        # –°—á–∏—Ç–∞–µ–º TF –∏ TF-IDF\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()  # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "            total_words = len(words)\n",
    "            word_count = {word: words.count(word) for word in set(words)}  # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤\n",
    "            \n",
    "            for word, count in word_count.items():\n",
    "                tf = count / total_words  # TF = (—á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ) / (–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)\n",
    "                tfidf_value = tf * idf.get(word, 0)  # TF-IDF = TF * IDF\n",
    "                if doc_id not in self.tfidf:\n",
    "                    self.tfidf[doc_id] = {}\n",
    "                self.tfidf[doc_id][word] = tfidf_value  # –°–æ—Ö—Ä–∞–Ω—è–µ–º TF-IDF\n",
    "\n",
    "    def get_tfidf(self, word, document):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ–º TF-IDF –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\"\"\"\n",
    "        return self.tfidf.get(document, {}).get(word, 0.0)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 0.0, –µ—Å–ª–∏ —Å–ª–æ–≤–æ –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n",
    "\n",
    "    def get_tfidf_unstemmed(self, word, document):\n",
    "        \"\"\"\n",
    "        –ü–æ–ª—É—á–∞–µ–º TF-IDF –¥–ª—è –Ω–µ —Å—Ç–µ–º–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.\n",
    "        –°—Ç–µ–º–º–∏—Ä—É–µ–º —Å–ª–æ–≤–æ –∏ –∑–∞—Ç–µ–º –≤—ã–∑—ã–≤–∞–µ–º get_tfidf.\n",
    "        \"\"\"\n",
    "        if self.p:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Ç–µ–º–º–µ—Ä\n",
    "            word = self.p.stem(word)  # –°—Ç–µ–º–º–∏—Ä—É–µ–º —Å–ª–æ–≤–æ\n",
    "        return self.get_tfidf(word, document)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "docs = [\n",
    "    \"this is the first document\",\n",
    "    \"this document is the second document\",\n",
    "    \"the first document is here\",\n",
    "    \"third document\"\n",
    "]\n",
    "titles = [\"Doc 1\", \"Doc 2\", \"Doc 3\", \"Doc 4\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä IRSystem\n",
    "ir_system = IRSystem(docs, titles)\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\n",
    "ir_system.index()\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º TF-IDF\n",
    "ir_system.compute_tfidf()\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º TF-IDF –¥–ª—è —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
    "word = \"document\"\n",
    "document_id = 1  # –ù–∞–ø—Ä–∏–º–µ—Ä, \"Doc 2\"\n",
    "tfidf_value = ir_system.get_tfidf(word, document_id)\n",
    "print(f\"TF-IDF for word '{word}' in document {document_id}: {tfidf_value}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will implement `rank_retrieve`. This function returns sorted list of the top ranked documents for a given query. Right now it ranks documents according to their Jaccard similarity with the query, but you will replace this method of ranking with a ranking using the <b>cosine similarity</b> between the documents and query.\n",
    "    \n",
    "Remember to use ltc.lnn weighting, that is, ltc weighting for the document and lnn weighting for the query! This means that the query vector weights will just be $\\text{tf}_{t, q} = 1 + \\log_{10} count(t, q)$  with no IDF term or normalization (or $\\text{tf}_{t, q} = 0$ if the term is not present in the query), but we do normalize the document vector weights by the magnitude of the document vector (square root of the sum of squares of the tf-idf weights). Finally, the cosine scores are the dot product of the query vector and the document vectors. Refer to this [handout](http://web.stanford.edu/class/cs124/lec/CS124_IR_Handout.pdf) or lecture slides for a more detailed explanation.\n",
    "    \n",
    "Just to be clear, when we say normalize by \"document length\" or \"length of document\", we mean the length of the document vector, NOT the number of words in the actual text document. So, when you‚Äôre computing cosine similarity between the query and document, the document vector is the tf-idf document weights for all terms in the vocabulary. The document length would be the L2 norm of the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\n",
      "Calculating tf-idf...\n",
      "Ranked Results: [(0, 0.7846198901675911), (2, 0.46501153043911886), (1, 0.3132670537154066), (3, 0.18781639755086263)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class IRSystem:\n",
    "    def __init__(self, docs, titles, stemmer=None):\n",
    "        self.docs = docs  # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "        self.titles = titles  # –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        self.inv_index = {}  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n",
    "        self.vocab = set()  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "        self.tfidf = {}  # TF-IDF –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        self.p = stemmer  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–µ–º–º–µ—Ä\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\"\"\"\n",
    "        print(\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è...\")\n",
    "        inv_index = {}\n",
    "\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()\n",
    "            self.vocab.update(words)\n",
    "            for pos, word in enumerate(words):\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index\n",
    "\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å–ª–æ–≤–æ.\"\"\"\n",
    "        return sorted(self.inv_index.get(word, {}).keys())\n",
    "\n",
    "    def compute_tfidf(self):\n",
    "        print(\"Calculating tf-idf...\")\n",
    "        self.tfidf = {}\n",
    "        total_docs = len(self.docs)\n",
    "\n",
    "        idf = {}\n",
    "        for word in self.vocab:\n",
    "            num_docs_with_word = len(self.get_posting(word))\n",
    "            idf[word] = math.log(total_docs / (num_docs_with_word + 1))\n",
    "\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            words = doc.split()\n",
    "            total_words = len(words)\n",
    "            word_count = {word: words.count(word) for word in set(words)}\n",
    "            \n",
    "            for word, count in word_count.items():\n",
    "                tf = count / total_words\n",
    "                tfidf_value = tf * idf.get(word, 0)\n",
    "                if doc_id not in self.tfidf:\n",
    "                    self.tfidf[doc_id] = {}\n",
    "                self.tfidf[doc_id][word] = tfidf_value\n",
    "\n",
    "    def get_tfidf(self, word, document):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ–º TF-IDF –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\"\"\"\n",
    "        return self.tfidf.get(document, {}).get(word, 0.0)\n",
    "\n",
    "    def get_tfidf_vector(self, document_id):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\"\"\"\n",
    "        return [self.get_tfidf(word, document_id) for word in self.vocab]\n",
    "\n",
    "    def rank_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query (a list of words), return a rank-ordered list of\n",
    "        documents (by ID) and score for the query.\n",
    "        \"\"\"\n",
    "        scores = [0.0 for _ in range(len(self.titles))]\n",
    "        \n",
    "        # Step 1: Create a TF-IDF vector for the query\n",
    "        query_tf = {}\n",
    "        for word in query:\n",
    "            query_tf[word] = query_tf.get(word, 0) + 1\n",
    "            \n",
    "        total_query_words = len(query)\n",
    "        query_vector = {}\n",
    "        for word, count in query_tf.items():\n",
    "            tf = count / total_query_words  # Calculate TF for the query word\n",
    "            idf = math.log(len(self.docs) / (len(self.get_posting(word)) + 1))  # Calculate IDF\n",
    "            query_vector[word] = tf * idf  # Compute TF-IDF for the query\n",
    "\n",
    "        # Step 2: Compute cosine similarity for each document\n",
    "        for d in range(len(self.docs)):\n",
    "            doc_vector = self.get_tfidf_vector(d)  # Get TF-IDF vector for document d\n",
    "            cosine_similarity = self.cosine_similarity(query_vector, doc_vector)\n",
    "            scores[d] = cosine_similarity\n",
    "\n",
    "        # Step 3: Rank documents by scores\n",
    "        ranking = sorted(range(len(scores)), key=lambda x: scores[x], reverse=True)\n",
    "        results = [(ranking[i], scores[ranking[i]]) for i in range(min(10, len(ranking)))]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def cosine_similarity(self, query_vector, doc_vector):\n",
    "        \"\"\"Calculate the cosine similarity between query and document vectors.\"\"\"\n",
    "        dot_product = sum(query_vector.get(word, 0) * doc_vector[i] for i, word in enumerate(self.vocab))\n",
    "        query_magnitude = math.sqrt(sum(val ** 2 for val in query_vector.values()))\n",
    "        doc_magnitude = math.sqrt(sum(val ** 2 for val in doc_vector))\n",
    "\n",
    "        if query_magnitude == 0 or doc_magnitude == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (query_magnitude * doc_magnitude)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "docs = [\n",
    "    \"this is the first document\",\n",
    "    \"this document is the second document\",\n",
    "    \"the first document is here\",\n",
    "    \"third document\"\n",
    "]\n",
    "titles = [\"Doc 1\", \"Doc 2\", \"Doc 3\", \"Doc 4\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä IRSystem\n",
    "ir_system = IRSystem(docs, titles)\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\n",
    "ir_system.index()\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º TF-IDF\n",
    "ir_system.compute_tfidf()\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\n",
    "query = [\"document\", \"first\"]\n",
    "results = ir_system.rank_retrieve(query)\n",
    "print(\"Ranked Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add a few methods that given a string, will process and then return the list of matching documents for the different methods you have implemented. You do not need to add any additional code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem): \n",
    "    def process_query(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a query string, process it and return the list of lowercase,\n",
    "        alphanumeric, stemmed words in the string.\n",
    "        \"\"\"\n",
    "        # make sure everything is lower case\n",
    "        query = query_str.lower()\n",
    "        # split on whitespace\n",
    "        query = query.split()\n",
    "        # remove non alphanumeric characters\n",
    "        query = [self.alphanum.sub('', xx) for xx in query]\n",
    "        # stem words\n",
    "        query = [self.p.stem(xx) for xx in query]\n",
    "        return query\n",
    "\n",
    "    def query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by boolean_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.boolean_retrieve(query)\n",
    "\n",
    "    def phrase_query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by phrase_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.phrase_retrieve(query)\n",
    "\n",
    "    def query_rank(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of the top matching\n",
    "        documents, rank-ordered.\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.rank_retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "You can use the `run_tests` and `run_query` functions to test your code. `run_tests` tests how different components your search engine code perform on a small set of queries and checks whether or not it matches up with our solution's results. `run_query` can be used to test your code on individual queries. \n",
    "\n",
    "Note that the first run for either of these functions might take a little while, since it will stem all the words in every document create a directory named stemmed/ in ../data/ProjectGutenberg/. This is meant to be a simple cache for the stemmed versions of the text documents. Later runs will be much faster after the first run since all the stemming will already be completed. However, this means that **if something happens during this first run and it does not get through processing all the documents, you may be left with an incomplete set of documents in ../data/ProjectGutenberg/stemmed/. If this happens, simply remove the stemmed/ directory and re-run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(irsys):\n",
    "    print(\"===== Running tests =====\")\n",
    "\n",
    "    ff = open('./data/dev_queries.txt')\n",
    "    questions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "    ff = open('./data/dev_solutions.txt')\n",
    "    solutions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "\n",
    "    epsilon = 1e-4\n",
    "    for part in range(6):\n",
    "        points = 0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        prob = questions[part]\n",
    "        soln = json.loads(solutions[part])\n",
    "        \n",
    "\n",
    "        if part == 0:  # inverted index test\n",
    "            print(\"Inverted Index Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_word_positions(word, doc)\n",
    "                if sorted(guess) == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        if part == 1:  # get postings test\n",
    "            print(\"Get Postings Test\")\n",
    "            words = prob.split(\", \")\n",
    "            for i, word in enumerate(words):\n",
    "                num_total += 1\n",
    "                posting = irsys.get_posting_unstemmed(word)\n",
    "                if posting == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 2:  # boolean retrieval test\n",
    "            print(\"Boolean Retrieval Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 3:  # phrase query test\n",
    "            print(\"Phrase Query Retrieval\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.phrase_query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 4:  # tfidf test\n",
    "            print(\"TF-IDF Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_tfidf_unstemmed(word, doc)\n",
    "                if guess >= float(soln[i]) - epsilon and \\\n",
    "                        guess <= float(soln[i]) + epsilon:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 5:  # cosine similarity test\n",
    "            print(\"Cosine Similarity Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                ranked = irsys.query_rank(query)\n",
    "                top_rank = ranked[0]\n",
    "                if top_rank[0] == soln[i][0]:\n",
    "                    if top_rank[1] >= float(soln[i][1]) - epsilon and \\\n",
    "                            top_rank[1] <= float(soln[i][1]) + epsilon:\n",
    "                        num_correct += 1\n",
    "\n",
    "        feedback = \"%d/%d Correct. Accuracy: %f\" % \\\n",
    "                   (num_correct, num_total, float(num_correct) / num_total)\n",
    "\n",
    "        if part == 1:\n",
    "            if num_correct == num_total:\n",
    "                points = 2\n",
    "            elif num_correct >= 0.5 * num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        elif part == 2:\n",
    "            if num_correct == num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        else:\n",
    "            if num_correct == num_total:\n",
    "                points = 3\n",
    "            elif num_correct > 0.75 * num_total:\n",
    "                points = 2\n",
    "            elif num_correct > 0:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "\n",
    "        print(\"    Score: %d Feedback: %s\" % (points, feedback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Indexing...\n",
      "Calculating TF-IDF...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        self.docs = {}  # Mapping from doc ID to text\n",
    "        self.titles = []  # Titles of the documents\n",
    "        self.inv_index = defaultdict(lambda: defaultdict(list))  # Inverted index\n",
    "        self.vocab = set()  # Vocabulary set\n",
    "        self.tfidf = {}  # TF-IDF storage\n",
    "\n",
    "    def read_data(self, data_path):\n",
    "        \"\"\"Read documents from the specified directory.\"\"\"\n",
    "        print(\"Reading data...\")\n",
    "        for filename in os.listdir(data_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(data_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    doc_id = len(self.docs)\n",
    "                    self.docs[doc_id] = text\n",
    "                    self.titles.append(filename)\n",
    "                    self.vocab.update(re.findall(r'\\w+', text.lower()))  # Update vocabulary\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"Create an inverted index for the documents.\"\"\"\n",
    "        print(\"Indexing...\")\n",
    "        for doc_id, text in self.docs.items():\n",
    "            words = re.findall(r'\\w+', text.lower())\n",
    "            for pos, word in enumerate(words):\n",
    "                self.inv_index[word][doc_id].append(pos)\n",
    "\n",
    "    def compute_tfidf(self):\n",
    "        \"\"\"Compute the TF-IDF values for each word in each document.\"\"\"\n",
    "        print(\"Calculating TF-IDF...\")\n",
    "        total_docs = len(self.docs)\n",
    "        idf = {}\n",
    "\n",
    "        # Calculate IDF for each word\n",
    "        for word in self.vocab:\n",
    "            num_docs_with_word = len(self.inv_index[word])\n",
    "            idf[word] = math.log(total_docs / (num_docs_with_word + 1))  # Smoothing\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        for doc_id, text in self.docs.items():\n",
    "            words = re.findall(r'\\w+', text.lower())\n",
    "            tf = defaultdict(int)\n",
    "\n",
    "            for word in words:\n",
    "                tf[word] += 1\n",
    "\n",
    "            total_words = len(words)\n",
    "\n",
    "            for word in tf:\n",
    "                tf_value = tf[word] / total_words  # Term Frequency\n",
    "                tfidf_value = tf_value * idf[word]  # TF-IDF\n",
    "                if doc_id not in self.tfidf:\n",
    "                    self.tfidf[doc_id] = {}\n",
    "                self.tfidf[doc_id][word] = tfidf_value  # Store TF-IDF\n",
    "\n",
    "# Example usage\n",
    "irsys = IRSystem()\n",
    "irsys.read_data('./data/ProjectGutenberg')\n",
    "irsys.index()\n",
    "irsys.compute_tfidf()\n",
    "\n",
    "# Assuming you have a function to run tests\n",
    "# run_tests(irsys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Indexing...\n",
      "Calculating TF-IDF...\n",
      "Best matching documents to 'My very own query':\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        self.docs = {}  # Mapping from doc ID to text\n",
    "        self.titles = []  # Titles of the documents\n",
    "        self.inv_index = defaultdict(lambda: defaultdict(list))  # Inverted index\n",
    "        self.vocab = set()  # Vocabulary set\n",
    "        self.tfidf = {}  # TF-IDF storage\n",
    "\n",
    "    def read_data(self, data_path):\n",
    "        \"\"\"Read documents from the specified directory.\"\"\"\n",
    "        print(\"Reading data...\")\n",
    "        for filename in os.listdir(data_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(data_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    doc_id = len(self.docs)\n",
    "                    self.docs[doc_id] = text\n",
    "                    self.titles.append(filename)\n",
    "                    self.vocab.update(re.findall(r'\\w+', text.lower()))  # Update vocabulary\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"Create an inverted index for the documents.\"\"\"\n",
    "        print(\"Indexing...\")\n",
    "        for doc_id, text in self.docs.items():\n",
    "            words = re.findall(r'\\w+', text.lower())\n",
    "            for pos, word in enumerate(words):\n",
    "                self.inv_index[word][doc_id].append(pos)\n",
    "\n",
    "    def compute_tfidf(self):\n",
    "        \"\"\"Compute the TF-IDF values for each word in each document.\"\"\"\n",
    "        print(\"Calculating TF-IDF...\")\n",
    "        total_docs = len(self.docs)\n",
    "        idf = {}\n",
    "\n",
    "        # Calculate IDF for each word\n",
    "        for word in self.vocab:\n",
    "            num_docs_with_word = len(self.inv_index[word])\n",
    "            idf[word] = math.log(total_docs / (num_docs_with_word + 1))  # Smoothing\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        for doc_id, text in self.docs.items():\n",
    "            words = re.findall(r'\\w+', text.lower())\n",
    "            tf = defaultdict(int)\n",
    "\n",
    "            for word in words:\n",
    "                tf[word] += 1\n",
    "\n",
    "            total_words = len(words)\n",
    "\n",
    "            for word in tf:\n",
    "                tf_value = tf[word] / total_words  # Term Frequency\n",
    "                tfidf_value = tf_value * idf[word]  # TF-IDF\n",
    "                if doc_id not in self.tfidf:\n",
    "                    self.tfidf[doc_id] = {}\n",
    "                self.tfidf[doc_id][word] = tfidf_value  # Store TF-IDF\n",
    "\n",
    "    def query_rank(self, query_str):\n",
    "        \"\"\"Process the query and rank the documents.\"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.rank_retrieve(query)\n",
    "\n",
    "    def process_query(self, query_str):\n",
    "        \"\"\"Process the query string to extract words.\"\"\"\n",
    "        # Lowercase, split, and remove non-alphanumeric characters\n",
    "        query = query_str.lower()\n",
    "        query = re.findall(r'\\w+', query)\n",
    "        return query\n",
    "\n",
    "    def rank_retrieve(self, query):\n",
    "        \"\"\"Rank documents based on cosine similarity.\"\"\"\n",
    "        scores = [0.0] * len(self.titles)\n",
    "        \n",
    "        for d, words_in_doc in self.docs.items():\n",
    "            for word in query:\n",
    "                if word in self.inv_index:\n",
    "                    # Calculate score based on TF-IDF\n",
    "                    tfidf_value = self.tfidf[d].get(word, 0)\n",
    "                    scores[d] += tfidf_value\n",
    "\n",
    "        ranking = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        return ranking\n",
    "\n",
    "# Run any query you want!\n",
    "def run_query(query):\n",
    "    irsys = IRSystem()\n",
    "    irsys.read_data('./data/ProjectGutenberg')\n",
    "    irsys.index()\n",
    "    irsys.compute_tfidf()\n",
    "    \n",
    "    print(\"Best matching documents to '%s':\" % query)\n",
    "    results = irsys.query_rank(query)\n",
    "    for docId, score in results:\n",
    "        print(\"%s: %e\" % (irsys.titles[docId], score))\n",
    "\n",
    "# Example usage\n",
    "run_query(\"My very own query\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, you can run the cell below to see the titles of all the documents in our dataset. As sanity checks, you can try tailoring your queries in `run_query` to output certain titles and/or checking what IR system outputs against the list of titles to see if the results make sense (i.e. the book _Great Pianists on Piano Playing_ should probably be among the top results if the query is \"pianists play piano\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prints full list of book titles in dataset (in alphabetical order)\n",
    "for i in range(len(irsys.titles)):\n",
    "    print(\"%d. %s\" % (i + 1, irsys.titles[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval Systems and Search Engines\n",
    "\n",
    "Safiya Umoja Noble‚Äôs <a href=\"https://nyupress.org/9781479837243/algorithms-of-oppression/\">Algorithms of Oppression</a> (2018) provides insight into how search engines and information retrieval algorithms can exhibit substantial racist and sexist biases. Noble demonstrates how prejudice against black women is embedded into search engine ranked results. These biases are apparent in both Google search‚Äôs autosuggestions and the first page of Google results. In this assignment, we have explored numerous features that are built into information retrieval systems, like Google Search. \n",
    "\n",
    "How could the algorithms you built in this assignment contribute to enforcing real-world biases? \n",
    "\n",
    "How can we reduce data discrimination and algorithmic bias that perpetuate gender and racial inequalities in search results and IR system?\n",
    "\n",
    "Please provide a short response to each of these questions (1-2 paragraphs per question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def algorithmic_bias_in_IR_systems(self):\n",
    "        # TODO: Place your response into the response string below\n",
    "        response = \"\"\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're ready to submit, you can run the cell below to prepare and zip\n",
    "up your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa4.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa4.ipynb deps/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running on Google Colab, see the README for instructions on\n",
    "how to submit.\n",
    "\n",
    "__Best of luck!__\n",
    "\n",
    "__Some reminders for submission:__\n",
    "* If you have any extra files required for your implementation to work, make\n",
    " sure they are in a `deps/` folder on the same level as `pa4.ipynb` and\n",
    " include that folder in your submission zip file.\n",
    " * Make sure you didn't accidentally change the name of your notebook file,\n",
    " (it should be `pa4.ipynb`) as that is required for the autograder to work.\n",
    "* Go to Gradescope (gradescope.com), find the PA4 IR assignment and\n",
    "upload your zip file (`submission.zip`) as your solution.\n",
    "* Wait for the autograder to run (it should only take a minute or so) and check\n",
    "that your submission was graded successfully! If the autograder fails, or you\n",
    "get an unexpected score it may be a sign that your zip file was incorrect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
